---
title: "Stats 101C - proj"
author: "Alexis Adzich"
date: "Summer 2024"
output:
  pdf_document: default
---


```{r, echo = FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidymodels)
library(tidyverse)
```

+ Use read_csv to read in the data. 
```{r}
train <- read_csv("ucla-stats-101-c-2024-su-regression/train.csv")
#train<- read_csv("Desktop/101c/ucla-stats-101-c-2024-su-regression/train.csv")
```

+ remove the variable `order_totals` as it will allow you to predict `log_total` perfectly

```{r}
train <- train %>% select(!order_totals)
train
```

+ Fold the training data into a 10-fold cross-validation set. Stratify on `log_total`. It is recommended to set a seed before your v-fold operation.

```{r}
set.seed(1000)
amz_folds <- vfold_cv(train, v = 10, strata = log_total)
amz_folds
```


+ TUNING - random forest trees

```{r}
#install.packages("ranger")
amz_rf_tune_model <- rand_forest(trees = tune()) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")
```

+ Create a workflow. Add a very simple formula to predict the outcome variable `log_total` based on all other variables.

```{r}
amz_recipe <- recipe(log_total~., data= train)
  #step_corr(all_numeric(), threshold = tune()) %>% 
  #step_normalize(all_numeric()) 

#amz_tune_model <- decision_tree


amz_rf_wflow <- 
  workflow() %>% 
  add_model(amz_rf_tune_model) %>% 
  add_recipe(amz_recipe)

  
amz_reg_tune <- 
  amz_rf_wflow %>% 
  update_model(amz_rf_tune_model)
amz_reg_tune

```
``` {r}
# tuning on trees -- did 50 combos
amz_grid <- grid_random(parameters(amz_rf_tune_model),
                        size = 50)


tuning <- amz_rf_wflow %>% 
  tune_grid(resamples = amz_folds,
            grid = amz_grid)
tuning

```

```{r}
tuning %>% 
  collect_metrics()
```

```{r}
autoplot(tuning)
show_best(tuning)
# fairly unconclusive -- bascially anything over 500 trees same
```


```{r}
amz_rf_model <- rand_forest(trees = 827) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")
#amz_recipe <- recipe(log_total~., data= train) 
#  step_corr(all_numeric(), threshold = .95) %>% 
#  step_normalize(all_numeric()) 

#amz_tune_model <- decision_tree
amz_rec_baked <- amz_recipe %>% 
  prep(training = train) %>% 
  bake(new_data = test)

amz_rf_wflow <- 
  workflow() %>% 
  add_model(amz_rf_model) %>% 
  add_recipe(amz_recipe)

amz_rf_res <- 
  amz_rf_wflow %>% 
  fit_resamples(resamples = amz_folds) 
```


+ Use `collect_metrics()` to get an estimate of the `rmse` for this random forest model. 

```{r}
collect_metrics(amz_rf_res)
```


```{r}
amz_rf_fit <- amz_rf_wflow %>% fit(data = train)
```

```{r}
# print the resulting fit for the purpose of grading
amz_rf_fit
```


+ Read in the test data

```{r}
test <- read_csv("ucla-stats-101-c-2024-su-regression/test.csv")
test
```


```{r}
amz_rf_test_res <- predict(amz_rf_fit, new_data = test)
amz_rf_test_res <- amz_rf_test_res %>% 
  bind_cols(id = test$id) %>% 
  relocate(id) %>% 
  rename(log_total = .pred)
```



```{r}
head(amz_rf_test_res, 15)
```


```{r}
write_csv(amz_rf_test_res, "rf_preds.csv")
```



```{r}
# tuning everything - mtry, trees, and regularization
rf_spec <- rand_forest(mtry = tune(), trees = tune()) %>% 
  set_engine("ranger", regularization.factor = tune("regularization")) %>% 
  set_mode("regression")

rf_param <- extract_parameter_set_dials(rf_spec)

# tried using pca, didnt seem to do much
pca_rec <- recipe(log_total~., data = train) %>% 
  step_normalize(contains("count")) %>% 
  step_pca(contains("count"), threshold = .95)

wflow <- workflow() %>% 
  add_model(rf_spec) %>% 
  add_recipe(pca_rec)
  
updated_param <- wflow %>% 
  extract_parameter_set_dials() %>% 
  finalize(train)

#updated_param
#penalty(trans = NULL, range = 10^c(-10, 0))

reg_tune <- wflow %>% 
  tune_grid(
    amz_folds,
    grid = updated_param %>% grid_regular(levels = 100)
  )
reg_tune
```

```{r}
autoplot(reg_tune)
show_best(reg_tune)

```



```{r}
rf_spec <- rand_forest(mtry = tune(), trees = tune()) %>% 
  set_engine("ranger", regularization.factor = tune("regularization")) %>% 
  set_mode("regression")

rf_param <- extract_parameter_set_dials(rf_spec)

#pca_rec2 <- recipe(log_total~., data = train) %>% 
#  step_novel(all_nominal_predictors()) %>% 
#  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>% 
#  step_zv(all_predictors())

norm_pca_rec2 <- recipe(log_total ~., data = train) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(q_demos_state, one_hot = TRUE) %>% 
  step_pca(all_numeric_predictors(), threshold = .95)


wflow <- workflow() %>% 
  add_model(rf_spec) %>% 
  add_recipe(norm_pca_rec2)
  
updated_param <- wflow %>% 
  extract_parameter_set_dials() %>% 
  finalize(train)


reg_tune5 <- wflow %>% 
  tune_grid(
    amz_folds,
    grid = updated_param %>% grid_random(size = 5)
  )
reg_tune5

```

```{r}
autoplot(reg_tune2)
show_best(reg_tune2)
```


```{r}
rf_spec <- rand_forest(mtry = 7, trees = 1564) %>% 
  set_engine("ranger", regularization.factor = .883) %>% 
  set_mode("regression")

??rand_forest
rf_spec <- rand_forest(mtry,
                       )
rf_param <- extract_parameter_set_dials(rf_spec)

pca_rec2 <- recipe(log_total~., data = train) 
#  step_normalize(contains("count")) %>% 
#  step_pca(contains("count"), threshold = .95)

wflow <- workflow() %>% 
  add_model(rf_spec) %>% 
  add_recipe(pca_rec2)
  
updated_param <- wflow %>% 
  extract_parameter_set_dials() %>% 
  finalize(train)


amz_rf_res <- 
  wflow %>% 
  fit_resamples(resamples = amz_folds) 

collect_metrics(amz_rf_res)
```


```{r}
amz_rf_fit <- wflow %>% fit(data = train)
test <- read_csv("ucla-stats-101-c-2024-su-regression/test.csv")

amz_rf_test_res <- predict(amz_rf_fit, new_data = test)

amz_rf_test_res <- amz_rf_test_res %>% 
  bind_cols(id = test$id) %>% 
  relocate(id) %>% 
  rename(log_total = .pred)

amz_rf_test_res
```


```{r}
write_csv(amz_rf_test_res, "rf_preds_reg_trees.csv")
```

```{r}
install.packages("xgboost")
```

```{r}
xgboost_rec <- recipe(log_total ~., data = train) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>% 
  #step_zv(all_predictors())
  step_pca(all_numeric_predictors(), threshold = .9)


xgboost_rec2 <- recipe(log_total ~., data = train) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(q_demos_state, one_hot = TRUE) %>% 
  step_pca(all_numeric_predictors(), threshold = .9)


xgboost_spec <- 
  boost_tree(trees = 1000,
             min_n = tune(),
             tree_depth = tune(),
             learn_rate = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("xgboost")

xgboost_wflow <-
  workflow() %>% 
  add_recipe(xgboost_rec2) %>% 
  add_model(xgboost_spec) 

params <- xgboost_wflow %>% 
  extract_parameter_set_dials() %>% 
  finalize(train)

set.seed(1000)
xgboost_tune <- tune_grid(xgboost_wflow,
                          resamples = amz_folds,
                          grid = params %>% grid_random(size = 5))
autoplot(xgboost_tune)
```


```{r}
#normalized_rec <- 
#   recipe(log_total ~ ., data = train) %>% 
  # step_normalize(all_numeric_predictors())  %>% 
#  step_dummy(q_demos_state) %>% 
#  step_pca(all_numeric_predictors(), threshold = .9)
xgboost_rec <- recipe(log_total ~., data = train) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(q_demos_state, one_hot = TRUE) %>% 
  step_pca(all_numeric_predictors(), threshold = .9)

xgboost_spec <- 
  boost_tree(trees = 1334,
             min_n = 16,
             tree_depth = 6,
             learn_rate = .08,
             sample_size = 0.5) %>% 
  set_mode("regression") %>% 
  set_engine("xgboost")

xgboost_wflow <-
  workflow() %>% 
  add_recipe(xgboost_rec) %>% 
  add_model(xgboost_spec) 

xgboost_res <- 
  xgboost_wflow %>% 
  fit_resamples(resamples = amz_folds) 

collect_metrics(xgboost_res)
```

```{r}
xgboost_fit <- xgboost_wflow %>% fit(data = train)
#test <- read_csv("ucla-stats-101-c-2024-su-regression/test.csv")

xgboost_test_res <- predict(xgboost_fit, new_data = test)

xgboost_test_res <- xgboost_test_res %>% 
  bind_cols(id = test$id) %>% 
  relocate(id) %>% 
  rename(log_total = .pred)

xgboost_test_res
```
```{r}
#write_csv()
```

```{r}
norm_rec <- recipe(log_total ~., data = train) %>% 
  step_dummy(all_nominal()) %>% 
  step_center(all_predictors()) %>% 
  step_scale(all_predictors()) %>% 
  prep(training = train)

glmn_fit <- linear_reg(penalty = .001, mixture= .5) %>% 
  set_engine("glmnet") %>% 
  fit(log_total ~., data = bake(norm_rec, new_data= NULL))
glmn_fit

test_normalized <- bake(norm_rec, new_data = test, all_predictors())



test <- read_csv("ucla-stats-101-c-2024-su-regression/test.csv")

glmnet_norm_results <- predict(glmn_fit, new_data = test_normalized)

glmnet_norm_results <- glmnet_norm_results %>% 
  bind_cols(id = test$id) %>% 
  relocate(id) %>% 
  rename(log_total = .pred)

glmnet_norm_results
```


```{r}
write_csv(glmnet_norm_results, "glmnet_norm_preds.csv")

```


```{r}
train
tidymodels_prefer()
# distribution of the log total prices
ggplot(train, aes(x = log_total)) +
geom_histogram(bins = 50, col= "white") + theme_bw()
```

```{r}
#state is categorical, fairly even so don't need to apply step other
# but should convert into dummy variable
ggplot(train, aes(x = q_demos_state)) +
geom_bar(col= "white") + theme_bw()
```


```{r}
glimpse(train)

normalized_rec <- recipe(log_total ~., data = train) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(q_demos_state)

poly_recipe <- normalized_rec %>% 
  step_poly(all_predictors()) %>% 
  step_interact(~all_predictors():all_predictors())

```

```{r}
#install.packages("rules")
#install.packages("baguette")
library(rules)
library(baguette)

linear_reg_spec <- linear_reg(penalty = tune(), mixture = tune()) %>% 
  set_engine("glmnet")

nnet_spec <- 
   mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %>% 
   set_engine("nnet", MaxNWts = 2600) %>% 
   set_mode("regression")

mars_spec <- 
   mars(prod_degree = tune()) %>%  #<- use GCV to choose terms
   set_engine("earth") %>% 
   set_mode("regression")

svm_r_spec <- 
   svm_rbf(cost = tune(), rbf_sigma = tune()) %>% 
   set_engine("kernlab") %>% 
   set_mode("regression")

svm_p_spec <- 
   svm_poly(cost = tune(), degree = tune()) %>% 
   set_engine("kernlab") %>% 
   set_mode("regression")

knn_spec <- 
   nearest_neighbor(neighbors = tune(), dist_power = tune(), weight_func = tune()) %>% 
   set_engine("kknn") %>% 
   set_mode("regression")

cart_spec <- 
   decision_tree(cost_complexity = tune(), min_n = tune()) %>% 
   set_engine("rpart") %>% 
   set_mode("regression")

bag_cart_spec <- 
   bag_tree() %>% 
   set_engine("rpart", times = 50L) %>% 
   set_mode("regression")

rf_spec <- 
   rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
   set_engine("ranger") %>% 
   set_mode("regression")

xgb_spec <- 
   boost_tree(tree_depth = tune(), learn_rate = tune(), loss_reduction = tune(), 
              min_n = tune(), sample_size = tune(), trees = tune()) %>% 
   set_engine("xgboost") %>% 
   set_mode("regression")
cubist_spec <- 
   cubist_rules(committees = tune(), neighbors = tune()) %>% 
   set_engine("Cubist")


nnet_param <- 
   nnet_spec %>% 
   extract_parameter_set_dials() %>% 
   update(hidden_units = hidden_units(c(1, 27)))
```

```{r}
normalized <- 
   workflow_set(
      preproc = list(normalized = normalized_rec), 
      models = list(SVM_radial = svm_r_spec, SVM_poly = svm_p_spec, 
                    KNN = knn_spec, neural_network = nnet_spec)
   )
normalized

normalized <- 
   normalized %>% 
   option_add(param_info = nnet_param, id = "normalized_neural_network")
normalized
```

```{r}
model_vars <- 
   workflow_variables(outcomes = log_total, 
                      predictors = everything())

no_pre_proc <- 
   workflow_set(
      preproc = list(simple = model_vars), 
      models = list(MARS = mars_spec, CART = cart_spec, CART_bagged = bag_cart_spec,
                    RF = rf_spec, boosting = xgb_spec, Cubist = cubist_spec)
   )
no_pre_proc

with_features <- 
   workflow_set(
      preproc = list(full_quad = poly_recipe), 
      models = list(linear_reg = linear_reg_spec, KNN = knn_spec)
   )
```

```{r}
all_workflows <- 
   bind_rows(no_pre_proc, normalized, with_features) %>% 
   # Make the workflow ID's a little more simple: 
   mutate(wflow_id = gsub("(simple_)|(normalized_)", "", wflow_id))
all_workflows
```

```{r}
#install.packages("earth")
grid_ctrl <-
   control_grid(
      save_pred = TRUE,
      parallel_over = "everything",
      save_workflow = TRUE
   )

grid_results <-
   all_workflows %>%
   workflow_map(
      seed = 1000,
      resamples = amz_folds,
      grid = 1,
      control = grid_ctrl
   )
#show_notes(.Last.tune.result)
```

```{r}
grid_results %>% 
   rank_results() %>% 
   filter(.metric == "rmse") %>% 
   select(model, .config, rmse = mean, rank)
```

```{r}
autoplot(
   grid_results,
   rank_metric = "rmse",  # <- how to order models
   metric = "rmse",       # <- which metric to visualize
   select_best = TRUE     # <- one point per workflow
) +
   geom_text(aes(y = mean - 1/2, label = wflow_id), angle = 90, hjust = 1) +
   lims(y = c(3.5, 9.5)) +
   theme(legend.position = "none")

```

```{r}
normalized_rec <- 
   recipe(log_total ~ ., data = train) %>% 
  # step_normalize(all_numeric_predictors())  %>% 
  step_dummy(q_demos_state) %>% 
  step_pca(all_numeric_predictors(), threshold = .9)

#poly_recipe <- 
 ##  normalized_rec %>% 
 #  step_poly(all_numeric_predictors()) %>% 
 #  step_interact(~ all_predictors():all_predictors())
rf_spec_tuned <- rand_forest(trees = 1000) %>% 
  set_engine("ranger", regularization.factor = .5) %>% 
  set_mode("regression")
wflow <- workflow() %>% 
  add_model(rf_spec_tuned)  %>% 
  add_recipe(normalized_rec)
rf_res <- wflow %>% fit_resamples(resamples = amz_folds)
collect_metrics(rf_res)
```
```{r}
#install.packages("reshape2")
library(reshape2)
ggplot(data = melt(cor(train[,-1])), aes(x = Var1, y = Var2,fill=value)) + 
  geom_tile()

install.packages("heatmaply")
library(heatmaply)
heatmaply_cor(x = cor(train[,-1]), xlab = "Features", 
              ylab = "Features")
```

